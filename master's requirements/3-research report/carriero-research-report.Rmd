---
title: "**Pilot Study: Assessing the impact of class imbalance on the performance of prediction models developed for dichotomous risk prediction.\\vspace{0.25in}**"
author: "Alex Carriero"
date: "January 10, 2022 \\vspace{3.5in}"
output: 
  pdf_document:
    # citation_package: natbib
    citation_package: biblatex
    extra_dependencies: "subfig"

bibliography: "rr.bib"
header-includes: 
   \usepackage[autocite = superscript, backend=biber, style=numeric,sorting=none, natbib = true]{biblatex}
   \usepackage[labelfont=bf]{caption}
   # \usepackage[super, compress]{natbib}
   # \usepackage{floatrow}
   # \floatsetup[figure]{capposition=top}
---

**Supervisors**:\
\
Maarten van Smeden, Utrecht Medical Center Utrecht\
Kim Luijken, Utrecht Medical Center Utrecht\
Ben van Calster, Catholic Univeristy of Leuven and Leiden University Medical Center\
\
**Program**: Methodology and Statistics for Behavioral, BioMedical, and Social Sciences.\
\
**Host Institution**: Julius Center for Health Science and Primary Care, UMC.\
\
**Candidate Journal**: Statistics in Medicine.\
\
**Word Count**: 2500\

\newpage

## 1. Introduction

a nice introduction: 

1. General background to set the scene, working to the specific topic of your thesis at the final sentence.

Prediction modelling in medicine is receiving increasing attention.  

While an abundance of prediction models exist - few are clinically useful. 

This is because of calibration is the achilles heel. 

Harder to develop a good prediction model in medicine than it sounds. 

--- 

Major obstacle to developing good prediction models in medicine is class imbalance. 

Known to degrade the predictive performance of the models -- less information about the class we are most interested in. 





2. Summary of existing work on the topic you're addressing + exact formulation of knowledge gap.





3. Why this work will address the knowledge gap and how

In this research project, we aim to determine the best practices for handling class imbalance when developing clinical prediction models for dichotomous risk prediction.  The harms of imbalance corrections have been previously demonstrated for clinical prediction models developed using logistic regression[@ruben].  

We begin this project with a pilot study.  The aim of this pilot study is to demonstrate the baseline performance of various prediction models developed in the presence of class imbalance. We do not pre-process data to correct for class imbalance.  In this pilot study, we aim to answer the questions: how does class imbalance affect the performance of clinical prediction models developed using various classification algorithms?

\newpage 

## 2. Methods

In this paper, we implemented a simulation study to investigate the effect of class imbalance on the performance of various classification algorithms. We adhere to the ADEMP guidelines for the design and reporting of our simulation study [@tim_morris].\

**Aim**

The aim of this pilot study was to investigate the effect of class imbalance on the performance of six commonly used classification algorithms.  In particular, we aimed to assess the out-of-sample predictive performance of prediction models trained with various classification algorithms, when the event fraction was varied across three levels and no imbalance corrections were implemented.\

**Data-Generating Mechanism**

Data for each class was generated independently from two distinct multivariate normal distributions:\
\
      Class 0: $\mathbf{X} \sim mvn( \pmb{\mu_0}, \pmb{\Sigma_0})$ = $mvn(\pmb{0}, \pmb{\Sigma_0})$\
\
      Class 1: $\mathbf{X} \sim mvn( \pmb{\mu_1}, \pmb{\Sigma_1})$ = $mvn(\pmb{\Delta_\mu}, \pmb{\Sigma_0} - \pmb{\Delta_\Sigma})$\

The parameters (mean vector and covariance matrix) of these data generating distributions were distinct between the classes. In the formulae above, $\pmb{\Delta_\mu}$ refers to the vector housing the difference in predictor means between the two classes. Similarly, $\pmb{\Delta_\Sigma}$ refers to the matrix housing the difference in predictor variances/covariances between the classes.\
\
In class 0, all predictor means were fixed to zero and all variances were fixed to 1. In class 1, all means were non-zero and are stored in the vector $\pmb{\Delta_\mu}$. There was no variation in means among predictors within a class, thus, every element in the vector $\pmb{\Delta_\mu}$ is equivalent; denoted by $\delta_\mu$. Similarly, there was no variation in predictor variances within a class, so every diagonal element in $\pmb{\Delta_\Sigma}$ is equivalent; diagonal elements are denoted by $\delta_\Sigma$.\
\
Finally, $80$% of the predictors were allowed to covary. All correlations among predictors in each class were set to $0.2$.   To ensure the correlation of predictors was not stronger in one class, the correlation matrices of the two classes were fixed to be equal.  This was accomplished by computing the off-diagonal elements of $\pmb{\Delta_\Sigma}$  such that the correlation matrices between the two classes were equivalent. Note, the covariance matrices were *not* equivalent between the classes. For example, in scenario where we have 8 predictors:\
\
mean and covariance structure for class 0, \begin{equation*}
\pmb{\mu_0} = \begin{bmatrix}
 \\ 0 \\ 0 \\ 0\\ 0 \\ 0 \\ 0 \\ 0 \\ 0
\end{bmatrix}, \pmb{\Sigma_0} = \begin{bmatrix}
1   & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0 & 0\\
0.2 & 1   & 0.2 & 0.2 & 0.2 & 0.2 & 0 & 0\\
0.2 & 0.2 & 1   & 0.2 & 0.2 & 0.2 & 0 & 0\\
0.2 & 0.2 & 0.2   & 1 & 0.2 & 0.2 & 0 & 0\\
0.2 & 0.2 & 0.2   & 0.2 & 1 & 0.2 & 0 & 0\\
0.2 & 0.2 & 0.2   & 0.2 & 0.2 & 1 & 0 & 0\\
0   & 0   & 0     &  0 & 0    & 0 & 1 & 0\\
0   & 0   & 0     &  0 & 0    & 0 & 0 & 1\\
\end{bmatrix}
\end{equation*}

mean and covariance structure for class 1, \begin{equation*}
\pmb{\mu_1} = \begin{bmatrix}
 \\ \delta_\mu \\ \delta_\mu \\ \delta_\mu \\ \delta_\mu \\ \delta_\mu \\ \delta_\mu \\ \delta_\mu \\ \delta_\mu
\end{bmatrix}, \pmb{\Sigma_1} = \begin{bmatrix}
1 - \delta_\Sigma   & z & z & z & z & z & 0 & 0\\
z & 1 - \delta_\Sigma   & z & z & z & z & 0 & 0\\
z & z & 1 - \delta_\Sigma   & z & z & z & 0 & 0\\
z & z & z   & 1 - \delta_\Sigma & z & z & 0 & 0\\
z & z & z   & z & 1 - \delta_\Sigma & z & 0 & 0\\
z & z & z   & z & z & 1 - \delta_\Sigma & 0 & 0\\
0   & 0   & 0     &  0 & 0    & 0 & 1 - \delta_\Sigma & 0\\
0   & 0   & 0     &  0 & 0    & 0 & 0 & 1 - \delta_\Sigma\\
\end{bmatrix}
\end{equation*}.\
\
Here, $z = (1-\delta_\Sigma)*0.2$, to ensure the correlation matrices of the two classes were equivalent.\
\
In the study we investigated simulated data to reflect three unique scenarios.  This was accomplished by varying the event fraction through the set {0.5, 0.2, 0.02}. For all  scenarios, data for 8 predictors was generated and the sample size ($N$) was determined as the minimum sample size required for the prediction model using the R package `pmsampsize` [@pmsampsize].\
\
Under the assumption of normality for all predictors (in each class), the  $\Delta C$ Statistic of the data can be expressed as a function of $\pmb{\Delta_\mu}$, $\pmb{\Sigma_0}$ and $\pmb{\Sigma_1}$ [@mvauc].  For every scenario, the parameter values for the data generating distributions ($\delta_\mu$ and $\delta_\Sigma$) were selected to generate a $\Delta C$ Statistic $=0.85$. Their values were computed analytically, based on the following formula [@mvauc]:

\begin{equation}
\Delta C = \Phi \left( \sqrt{\pmb{\Delta_\mu}{'}\  (\pmb{\Sigma_0} + \pmb{\Sigma_1})^{-1} \ \pmb{\Delta_{\mu}}} \right).
\end{equation}
\
In equation (1), $\Phi$ represents the cumulative density function (cdf) of the standard normal distribution; $\pmb{\Delta_\mu}$, $\pmb{\Sigma_0}$ and $\pmb{\Sigma_1}$ maintain their previous definitions. To ensure a unique solution $\delta_\Sigma$ was  fixed at 0.3 for each scenario, while equation (1) was solved to yield the appropriate value of $\delta_\mu$ in each scenario.\
\
Finally, given that data for each class were generated independently, we had direct control over how many observations were generated under each class. The number of observations from the positive class ($n_1$) was sampled from the binomial distribution with probability equal to the specified event fraction. The number of observations in the negative class ($n_0$) was then be computed as $N - n_1$, where $N$ is the minimum sample size specified for the prediction model.\
\
Parameter values for the data generating distributions of the simulation scenarios are presented in Table 1.\


```{r, warning = F, message = F, echo = F}
library(tidyverse)
library(kableExtra)

read.csv("table.csv") %>%
  filter(npred == 8, n == "N")%>% 
  dplyr::select(ef, npred, ss, dmu, dsig, AUC)%>%
  rename("Event Fraction"    = ef, 
         "No. Predictors"    = npred,
         "N"                 = ss, 
         "$\\delta_\\mu$"    = dmu, 
         "$\\delta_\\Sigma$" = dsig, 
         "$\\Delta C$"       = AUC)%>%
  kbl(booktabs = T, escape = F,
      linesep = "\\addlinespace",
      caption = "Summary of the data generating parameters for each simulation scenario.")%>%
  kable_styling(full_width = F, latex_options ="hold_position", font_size = 10)
```

\newpage 

**Simulation Methods**\
\
Under each simulation scenario, $200$ data sets were generated. Each data set was comprised of training and validation data. The training and validation data were generated independently using identical data generating mechanisms; this was done to ensure a similar event fraction in the training and validation data. The validation data was generated to be 10x larger than the training set (Table 1).\
\
For each generated data set,  six prediction models were developed, each using a different classification algorithm. All prediction models were trained using the training data. Out-of-sample performance was then assessed using the validation data.\
\
Classification algorithms were selected based on a systematic review identifying common algorithms used to develop prediction models in a medical context [@constanza]. These algorithms include: logistic regression, support vector machine, random forest and XGBoost.  Additionally, based on literature summarizing common strategies to handle class imbalance [@kaur; @lp; @summary_m; @yu], we include two ensemble learning algorithms designed specifically to handle class imbalance: RUSBoost and and EasyEnsemble. Classification algorithms were implemented with their default hyper-parameters; no hyper-parameter tuning was conducted. All classification algorithms and the R packages used for their implementation are summarized in Table 2.\

```{r, echo = F, warning = F, message = F}
# Summary <- c("\\bf{Imbalance Corrections}", 
#              " ", " ", " ", " ",
#              "\\bf{Classification Algorithms}", 
#              " ", " ", " ", " ", " ", " ")

Name <- c(
          "Logistic Regression",
          "Support Vector Machine",
          "Random Forest",
          "XGBoost",
          "RUSBoost",
          "EasyEnsemmble")


Abbreviation <- c("LR", "SVM", "RF", "XG", "RB", "EE")

Implementation <- c( "base R \\autocite{r}", "e1071 \\autocite{esvm}", 
                     "randomForest \\autocite{rf}", "xgboost \\autocite{xg}",
                    "ebmc \\autocite{ebmc}", "iric \\autocite{iric}")

cbind(Name, Abbreviation, Implementation)%>%
  as.data.frame()%>%
  rename("R Package" = "Implementation", 
         "Classification Algorithm" = "Name")%>% 
  kable(booktabs = T, escape = F,
        linesep = "\\addlinespace",
        caption = "Summary of classification algorithms used in simulation study.") %>%
  kable_styling(full_width = F, latex_options ="hold_position", font_size = 10) %>% 
  column_spec(1, width = "20em") %>%
  column_spec(2, width = "7em")
  # add_indent(c(1:10), level_of_indent = 3) %>%
  # pack_rows("Imbalance Corrections", 1, 4) %>%
  # pack_rows("Classification Algorithms", 5, 10)
```

**Performance Measures**

Out-of-sample model performance was assessed using measures of calibration, discrimination and overall performance. All performance metrics were computed using the validation data.\
\
Calibration refers to the reliability of the risk predictions; it measures the agreement between the risk predictions and observed proportions in the data [@epi, @achilles, @nature]. A calibration plot is the recommended method for evaluating model calibration [@nature].   For each simulation iteration, we fitted a flexible calibration curve for all models, using loess regression. In a flexible calibration curve, when risk predictions (x-axis) correspond well to the observed proportions (y-axis), the curve follows a diagonal line ($y = x$) [@achilles]. In addition to the calibration plots, calibration intercept and slope were calculated.  With respect to calibration intercept and slope, good calibration is represented by values of 0 and 1, respectively.\
\
Discrimination refers to a model's ability to distinguish between the classes [@nature].  The concordance statistic was used to measure model discrimination  ($\Delta$C-statistic); computed using the R package `pROC` [@pROC]. This is the most common metric to asses discrimination and for dichotomous risk prediction, it is equivalent to the area under the Receiver Operating Characteristic (ROC) curve [@epi; @nature]. A model which perfectly discriminates between the classes will have a $\Delta$C-statistic of 1; the minimum value for this statistic is 0.5.\
\
Overall performance was measured by brier score. This metric reflects both model discrimination and calibration and is calculated according to the following formula [@epi]: 

\begin{equation}
\mathrm{Brier Score} = \frac{1}{N} \sum^{N}_{i = 1} (p_i - o_i)^2
\end{equation}

where $N$ is the sample size, $p_i$ represents the estimated probability for the $i$th individual and $o_i$ represents
the observed outcome (0 or 1) for the $i$th individual. In an ideal model, estimated probabilities approximate the observed outcome well for all individuals; ideal models produce a brier score near to zero.\
\
For empirical measures of model performance ($\Delta C$ statistic, brier score, calibration intercept and slope), the mean over all iterations and corresponding monte carlo error are reported. No measures of classification accuracy were considered.  Measures of classification accuracy require a decision threshold to be selected and in this simulation study, there is not sufficient context to motivate where to place a threshold.\

**Software**

All analyses were conducting using R version 4.1.2 [@r]. 

## 3. Results

Results are summarized in Table 3 and model performance visualizations are displayed in Figure 1.\
\
For approximately balanced data (event fraction $= 0.5$), all algorithms, except XG and EE, were well calibrated. While, on average, both XG and EE had calibration intercepts near zero, their average calibration slopes dramatically deviated from 1.  We see that for XG, the risk predictions above 0.5 overestimated true risk, while the risk predictions below 0.5 underestimated true risk (Figure 1(a)). In other words, the XG models resulted in risk estimates which were too extreme (calibration slope = $0.464$).  The opposite pattern is true for EE; EE produced risk estimates which were too moderate (calibration slope = $2.279$). In the balanced data scenario, SVM and LR had similar discrimination and overall performance and both outperformed the other algorithms.\

With the event fraction at 0.2, all algorithms exhibited worse calibration, on average, compared to the balanced data scenario.  While the LR, SVM and RF maintained adequate calibration in this scenario, XG, RB and EE all, on average, produced risk predictions that dramatically over-estimated true risk (Figure 1(b)). With respect to discrimination and overall performance, in this scenario, LR was the best performing algorithm.\

At an event fraction of 0.02, all algorithms exhibited miscalibration.  From Figure 1 (c), we see that for LR, SVM and RF, there was large variation in the calibration curves produced for each iteration of the simulation.  Meanwhile, for XG, RB, and EE, the calibration curves did not vary much across the iterations, rather, they exhibited a specific pattern of miscalibration: all risk predictions over-estimated true risk. With respect to discrimination and overall performance, in this scenario, LR was again, the best performing algorithm.\

Overall, as imbalance between the classes was magnified, model calibration deteriorated across all algorithms. Meanwhile, discrimination maintained relatively constant.  Interestingly, as imbalance between the classes was magnified, overall performance appeared to improve, especially for the LR and SVM models. This apparent improvement in overall performance is misleading and is the result of a poor choice in performance metric. 

```{r, echo = F, results = 'hide'}
# on average 

trivial <- c(rep(0, 1797))
outcome <- c(rep(1, 36), rep(0, 1797-36))

mean((outcome-trivial)^2)
```


```{r,  echo = F, warning = F, message = F}
four <- readRDS("sim_4.rds")
five <- readRDS("sim_5.rds")
six  <- readRDS("sim_6.rds")

stats <- cbind(four$stats, five$stats, six$stats)
stats <- as.data.frame(t(stats))
stats <- round(stats, digits = 3) # %>% as.matrix()

stats <-
  stats %>%
  mutate(auc_mean = paste0(format(round(auc_mean, 3), nsmall = 3), " (", format(round(auc_sd, 3), nsmall = 3), ")"),
         bri_mean = paste0(format(round(bri_mean, 3), nsmall = 3), " (", format(round(bri_sd, 3), nsmall = 3), ")"),
         int_mean = paste0(format(round(int_mean, 3), nsmall = 3), " (", format(round(int_sd, 3), nsmall = 3), ")"),
         slp_mean = paste0(format(round(slp_mean, 3), nsmall = 3), " (", format(round(slp_sd, 3), nsmall = 3), ")")) %>%
  select(auc_mean, bri_mean, int_mean, slp_mean) %>%
  as.matrix()

  # unite("$\\Delta$ C", 1:2, remove = T, sep = " ") %>%
  # unite("Brier Score", 2:3, remove = T,  sep = " ") %>%
  # unite("CI", 3:4, remove = T, sep = " ") %>%
  # unite("CS", 4:5, remove = T, sep = " ") %>%
  # as.matrix()

colnames(stats)<- c("$\\Delta C$ Statistic", "Brier Score", "Calibration Intercept", "Calibration Slope")
# colnames(stats)<- c(rep(c("mean", "sd"), 4))
rownames(stats)<- c(rep(c("LR", "SVM", "RF", 
                          "XG", "RB", "EE"), 3))
stats %>%
  kbl(booktabs = T, escape = F,
      linesep = c(" ", " ", "", " ", " ", 
                  "\\addlinespace \\addlinespace"),
      align = "lcccc",
      # col.names = c(rep(c("mean", "mc error"), 4)),
      caption = "Mean (monte carlo error) of performance metrics across $200$ iterations in each simulation scenario.")%>%
  kable_styling(full_width = T,  latex_options ="hold_position", font_size = 10) %>% 
  pack_rows("Event Fraction: 0.5",   1,  6) %>%
  pack_rows("Event Fraction: 0.2",   7, 12) %>%
  pack_rows("Event Fraction: 0.02", 13, 18) %>%
  add_indent(c(1:18), level_of_indent = 2) %>%
  column_spec(1, width = "14em") %>% 
  row_spec(0, bold=F) # %>%
  # add_header_above(c(" " = 1, "$ \\\\Delta$ C Statistic" = 2, "Brier Score" = 2, 
  #                  "Calibration Int." = 2, "Calibration Slope" = 2), escape = F, bold = T) # %>%
  # row_spec(0, bold=T)
```


```{r, echo = F, fig.align='center', out.height="25%", fig.ncol = 1, fig.cap="Visual representation of model calibration for each simulation scenario in the Pilot Study.", fig.subcap=c("Flexible calibration curves with event fraction: 0.5.", "Flexible calibration curves with event fraction: 0.2", "Flexible calibration curves with event fraction: 0.02")}
library(knitr)
include_graphics(c("images/plot_4.png", "images/plot_5.png", "images/plot_6.png"))
```


\newpage

## 4. Discussion

In this paper we investigated the impact of class imbalance on the performance of clinical prediction models developed using six different classification algorithms.  The results of this study highlighted the baseline performance of prediction models developed in the presence of class imbalance; no imbalance corrections were applied to the data before training prediction models. Overall, we saw that as the event fraction was decreased, models exhibited increased miscalibration. If clinicians intend to use risk predictions from a prediction model to inform medical decision the  reliability of the risk predictions (calibration) is important; models should calibrated to increase the clinical utility of prediction models [@achilles; @12days]. However, class imbalance is exceedingly common in medical data sets.  Future work will investigate how to best correct for class imbalance without compromising model calibration. 

We note two significant limitations to this study. First, brier score appeared to be an uninformative measure of overall performance when class imbalance was extreme.  With an event fraction of 0.02, a trivial majority classifier (a model that predicts everyone will belong to the majority class) would yield a brier score of 0.02.  Therefore, in our future work, we will utilize another metric of overall performance, such a re-scaled brier score which is known to be more informative in the presence of class imbalance [@epi]. Second, models developed with classification algorithms other than logistic regression may have performed worse than expected due to the lack of hyper-parameter tuning.  In this analysis all algorithms were implemented using their software defaults. In particular, RB and EE preformed substantially worse than expected.  These algorithms are designed specifically to handle class imbalance, yet, they did not out preform logistic regression in any scenario.  These algorithms had worse overall performance than a trivial majority classifier at the most extreme fraction. The relatively poor performance of these algorithms may be due to a lack of hyper-parameter tuning. \

*nice last paragraph* 
- introduce the full simulation 
- mention hyper parameter tuning, re-scaled brier -- maybe clinical utility



\newpage
