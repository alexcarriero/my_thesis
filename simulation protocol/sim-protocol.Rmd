---
title: "**Simulation Study Protocol \\vspace{0.25in}**"
subtitle: "Simulation(s) to assess the impact of class imbalance corrections on the calibration of prediction models."
author: "Alex Carriero"
date: "November 14, 2022"
output: pdf_document
geometry: margin = 40mm

bibliography: "sim-protocol.bib"
header-includes: 
  \usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
---
```{r, echo = F, message = F, warning = F}
library(tidyverse)
library(kableExtra)
```

\newpage

## 1  ADEMP 

### 1.1  Aim

We aim to assess the impact of common class imbalance corrections on the performance of clinical prediction models. In this study, we will systematically compare the effect of imbalance corrections on the performance of several clinical prediction models developed using various machine learning methods.  All models are developed for dichotomous risk prediction. In particular, we aim to determine if class imbalance corrections can lead to improved model performance without compromising model calibration. 

### 1.2  Data-Generating Mechanisms

~ insert data generating mechanism details ~

* discuss how control of auc is maintained, present mean (sd) auc for generated data sets 
* how are the interaction effects achieved

Imbalanced data will be simulated to reflect 27 (3 x 3 x 3) unique scenarios.  This is achieved by varying the following three properties of the data: number of predictors, event fraction and sample size.  The number of predictors will vary through the set {8,16,32} and event fraction, through the set {0.5, 0.2, 0.02}. The minimum sample size for the prediction model (N) will be computed according to formulae presented in @riley.  Sample size will then vary through the set {$\frac{1}{2}$N, N and 2N}.\

```{r, echo = F, message = F, warning = F}
factor <- c("No. of predictors", 
            "Event fraction", 
            "Sample Size")

levels <- c("$8$, $16$, $32$", 
            "$0.5$, $0.2$, $0.02$", 
            "$\\frac{1}{2}$$N$, $N$, $2N$")

x <- as.data.frame(cbind(factor, levels))
colnames(x) <- c("Factor", "Levels")

x %>% 
  kbl(booktabs = T, caption = "Summary of factors to be varied in data simulation.", linesep = "", escape = FALSE) %>% 
  kable_styling(full_width = T, latex_options ="hold_position") %>%
  row_spec(0, bold = T, background = "#FAE7B5") %>% 
  footnote(symbol = "N represents the minimum sample size for the prediction model.")
```

Under each scenario, $2000$ data sets will be generated. Data sets will be comprised of training and test data with a ratio of 10:1. For each data set, five imbalance corrections will be applied to the training set.  Subsequently, six prediction models will be developed for each of the imbalance corrected training sets.  In other words, for each data set, the training data will be used to train $5$ x $6$ $= 30$ imbalance correction - prediction model combinations.  Finally, out-of sample predictive performance will be assessed for each imbalance correction - prediction model combination using the test data.

### 1.3 Estimands

The focus of this study is the out-of-sample predictive performance of clinical prediction models for dichotomous risk prediction.

### 1.4  Methods

To investigate the effect of common class imbalance corrections on model performance, a full-factorial simulation design will be implemented.  Five imbalance corrections will be implemented for each of six classification algorithms. The classification algorithms and imbalance corrections we will include in our simulation are detailed in Tables 2 and 3 respectively.\
\
All models will be trained using training data sets.  Out-of-sample performance will be then be assessed using the test data.  

```{r, echo = F, warning = F, message = F}
Methods <- c("Random Under Sampling", 
             "Random Over Sampling", 
             "SMOTE", 
             "SMOTE-ENN", 
             "None"
             )

Package <-c("ROSE", 
            "ROSE", 
            "smotefamily", 
            "*IRIC", 
            "---")

Python <-c("imblearn", 
           "imblearn", 
           "imblearn", 
           "imblearn", 
           "---")

x <- as.data.frame(cbind(Methods, Package, Python))
colnames(x) <- c("Imbalance Correction", "R Package", "Python Library")

x %>% 
  kbl(booktabs = T, caption = "Summary of class imbalance corrections to be implemented.", linesep = "") %>% 
  kable_styling(full_width = T, latex_options ="hold_position") %>%
  row_spec(0, bold = T, background = "#FAE7B5") %>%
  footnote(symbol =  "IRIC package not available on CRAN")
```

```{r, echo = F, message = F, warning =F}
Methods <- c("Logistic Regression", 
             "Support Vector Machine", 
             "Random Forest", 
             "XG Boost", 
             "RUSBoost", 
             "EasyEnsemble"
             )

Package <-c("glmnet", 
            "e1701", 
            "randomForest", 
            "xgboost", 
            "ebmc", 
            "*IRIC")

Python <-c("scikit-learn", 
           "scikit-learn", 
           "scikit-learn", 
           "xgboost", 
           "imblearn", 
           "imblearn")

x <- as.data.frame(cbind(Methods, Package, Python))
colnames(x) <- c("Method", "R Package", "Python Library")

x %>% 
  kbl(booktabs = T, caption = "Summary of classification algorithms to be implemented.", linesep = "") %>% 
  kable_styling(full_width = T, latex_options = c("hold_position")) %>%
  row_spec(0, bold = T, background = "#FAE7B5") %>% 
  footnote(symbol =  "IRIC package not available on CRAN")
```

### 1.5 Performance Measures

Out-of-sample model performance will be assessed using measures of discrimination, accuracy and calibration.  Discrimination will be measured by area under the receiver operator curve ($\Delta$C-statistic). Four measures of accuracy will be reported: overall accuracy, Matthew's correlation coefficient, sensitivity and specificity.  Finally, calibration will be measured in terms of calibration intercept and slope.  

\newpage

## 2  Error Handling 


\newpage 

## References