---
title: "**Simulation Study Protocol \\vspace{0.25in}**"
subtitle: "Simulation(s) to assess the impact of class imbalance corrections on the calibration of prediction models."
author: "Alex Carriero"
date: "November 17, 2022"
output: pdf_document
geometry: margin = 36mm

bibliography: "sim-protocol.bib"
header-includes: 
  \usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
---
```{r, echo = F, message = F, warning = F}
library(tidyverse)
library(kableExtra)
```

\newpage

## 1  ADEMP 

### 1.1  Aim

We aim to determine the best practices for handling class imbalance when developing clinical prediction models developed for dichotomous risk prediction. Under a variety of realistic scenarios, four imbalance corrections and six classification algorithms will be used to train prediction models; models will then be systematically compared based on their out-of-sample predictive performance.\
\
We aim to identify any combination of imbalance correction and classification algorithm that can outperform a control model (model developed using the given classification algorithm with no imbalance correction). In particular, we aim to determine which (if any) imbalance corrections lead to improved model performance without compromising model calibration. 

### 1.2  Data-Generating Mechanisms

### 1.2.1 Scenarios

Imbalanced data will be simulated to reflect 27 (3 x 3 x 3) unique scenarios.  This is achieved by varying the following three properties of the data: number of predictors, event fraction and sample size.  The number of predictors will vary through the set {8,16,32} and event fraction, through the set {0.5, 0.2, 0.02}. The minimum sample size for the prediction model (N) will be computed according to formulae presented in @riley.  Sample size will then vary through the set {$\frac{1}{2}$N, N and 2N}.

```{r, echo = F, message = F, warning = F}
factor <- c("No. of predictors", 
            "Event fraction", 
            "Sample Size")

levels <- c("$8$, $16$, $32$", 
            "$0.5$, $0.2$, $0.02$", 
            "$\\frac{1}{2}$$N$, $N$, $2N$")

x <- as.data.frame(cbind(factor, levels))
colnames(x) <- c("Factor", "Levels")

x %>% 
  kbl(booktabs = T, caption = "Summary of factors to be varied in data simulation.", linesep = "", escape = FALSE) %>% 
  kable_styling(full_width = T, latex_options ="hold_position") %>%
  row_spec(0, bold = T, background = "#FAE7B5") %>% 
  footnote(symbol = "N represents the minimum sample size for the prediction model.")
```

Under each scenario, $2000$ data sets will be generated. Data sets will be comprised of training and test data such that the training data set is 10x larger than the test set.

### 1.2.1 Data Generating Mechanism 

Data for each class is generated independently from two distinct multivariate normal distributions:\
\
\ \ \ \ \ \ Class 0: $\mathbf{X} \sim mvn( \pmb{\mu_0}, \pmb{\Sigma_0})$ = $mvn(\pmb{0}, \pmb{\Sigma_0})$\
\
\ \ \ \ \ \ Class 1: $\mathbf{X} \sim mvn( \pmb{\mu_1}, \pmb{\Sigma_1})$ = $mvn(\pmb{\Delta_\mu}, \pmb{\Sigma_0} +\pmb{\Delta_\Sigma})$\

The parameters (mean vector and covariance matrix) of the data generating distributions are distinct between the classes. In the formulae above, $\pmb{\Delta_\mu}$ refers to the vector housing the difference in predictor means between the two classes.  Similarly, $\pmb{\Delta_\Sigma}$ refers to the matrix housing the difference in predictors variances/covariances between the two classes. $\pmb{\Delta_\Sigma}$ is a diagonal matrix; all predictor covariances will be equal between the classes.  

The parameter values for the data generating distributions in each class are selected to generated a $\Delta C$ Statistic $=$ **INSERT**.  For each scenario, the parameters of the data generating distribution are included in the Table 2. Mean and standard deviation estimates of AUC are calculated based on a small simulation,  in which 2000 data sets are generated. This is done to detail the expected mean and variation of AUC of data generated for each scenario in the full simulation study. 

```{r, echo =F}
ef   <- c(0.5, 0.2, 0.02)
pred <- c(8, 16, 32)
N    <- c("0.5N", "N", "2N")

grid <- expand.grid(ef, pred, N) %>%
  rename("Event Fraction" = Var1, 
         "No. Predictors" = Var2, 
         "Sample Size" = Var3) 

grid %>%
  select(-last_col()) %>%
  mutate("Delta Mean"  = c(rep(0, 27)), 
         "Delta Var" = c(rep(0, 27)),
         AUC   = c(rep(0, 27)), 
         SD    = c(rep(0, 27))) %>%
  kbl(booktabs = T, caption = "Summary of parameters used in data generating mechanism for all simulation scenarios") %>%
  kable_styling(full_width = T, latex_options ="hold_position") %>% 
  pack_rows("0.5N", 1,9) %>% 
  pack_rows("N", 10, 18) %>% 
  pack_rows("2N",19, 27)
```

### 1.3 Estimands

The focus of this study is the out-of-sample predictive performance of clinical prediction models for dichotomous risk prediction.

### 1.4  Methods

To investigate the effect of common class imbalance corrections on model performance, a full-factorial simulation design will be implemented.  Four imbalance corrections and one control (no correction) will be implemented for each of six classification algorithms. The classification algorithms and imbalance corrections we will include in our simulation are detailed in Tables 3 and 4 respectively.

```{r, echo = F, warning = F, message = F}
Methods <- c("Random Under Sampling", 
             "Random Over Sampling", 
             "SMOTE", 
             "SMOTE-ENN", 
             "None"
             )

Package <-c("ROSE", 
            "ROSE", 
            "smotefamily", 
            "*IRIC", 
            "---")

Python <-c("imblearn", 
           "imblearn", 
           "imblearn", 
           "imblearn", 
           "---")

x <- as.data.frame(cbind(Methods, Package, Python))
colnames(x) <- c("Imbalance Correction", "R Package", "Python Library")

x %>% 
  kbl(booktabs = T, caption = "Summary of class imbalance corrections to be implemented.", linesep = "") %>% 
  kable_styling(full_width = T, latex_options ="hold_position") %>%
  row_spec(0, bold = T, background = "#FAE7B5") %>%
  footnote(symbol =  "IRIC package not available on CRAN")
```

```{r, echo = F, message = F, warning =F}
Methods <- c("Logistic Regression", 
             "Support Vector Machine", 
             "Random Forest", 
             "XG Boost", 
             "RUSBoost", 
             "EasyEnsemble"
             )

Package <-c("glmnet", 
            "e1701", 
            "randomForest", 
            "xgboost", 
            "ebmc", 
            "*IRIC")

Python <-c("scikit-learn", 
           "scikit-learn", 
           "scikit-learn", 
           "xgboost", 
           "imblearn", 
           "imblearn")

x <- as.data.frame(cbind(Methods, Package, Python))
colnames(x) <- c("Method", "R Package", "Python Library")

x %>% 
  kbl(booktabs = T, caption = "Summary of classification algorithms to be implemented.", linesep = "") %>% 
  kable_styling(full_width = T, latex_options = c("hold_position")) %>%
  row_spec(0, bold = T, background = "#FAE7B5") %>% 
  footnote(symbol =  "IRIC package not available on CRAN")
```
\
In summary, for each data set, five imbalance corrections (four + one control) will be applied to the training set.  Subsequently, six prediction models will be developed for each of the imbalance corrected training sets.  In other words, each data set will result in: 5 corrected training sets x 6 classification algorithms = 30 prediction models.  All models will be trained using training data sets.  Out-of-sample performance will be then be assessed using the test data. 

\newpage

### 1.5 Performance Measures

Out-of-sample model performance will be assessed using measures of discrimination, accuracy and calibration. 

**Discrimination**:\
\
Discrimination will be measured by area under the receiver operator curve ($\Delta$C-statistic). 

**Accuracy**:\
\
Four measures of accuracy will be reported:

- Overall accuracy 
- Matthew's correlation coefficient
- Sensitivity 
- Specificity 

For all measures of accuracy, a decision threshold must be imposed. \
\
~ what decision threshold? ~


**Calibration:**\
\
Calibration will be measured in terms of calibration intercept and slope.  Model calibration will be visualized using flexible model calibration curves.\

\newpage

## 2  Error Handling 


\newpage 

## References